<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://enyu-zhao.github.io">Enyu Zhao</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://vedant2311.github.io">Vedant Raval</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://www.hejiazhang.me">Hejia Zhang</a>,</span>
            </span>
            <span class="author-block">
              <a href="https://pointscoder.github.io">Jiageng Mao</a>,</span>
            </span>
            <span class="author-block">
              <a href="https://zshanggu.github.io">Zeyu Shangguan</a>,</span>
            </span>
            <span class="author-block">
              <a href="https://yuewang.xyz">Yue Wang</a>,</span>
            </span>
            <span class="author-block">
              <a href="https://danielseita.github.io">Daniel Seita</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Southern California,</span>
            <span class="author-block"><sup>*</sup>Equal Contibution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-Language Models (VLMs) have revolutionized artificial intelligence and robotics due to their commonsense reasoning capabilities.
            In robotic manipulation, VLMs are used primarily as high-level planners, but recent work has also studied their lower-level reasoning ability, which refers to making decisions about precise robot movements.
            However, the community currently lacks a clear and common benchmark that can evaluate how well VLMs can aid low-level reasoning in robotics.
            Consequently, we propose a novel benchmark, <b>ManipBench</b>, to evaluate the low-level robot manipulation reasoning capabilities of VLMs across various dimensions, including how well they understand object-object interactions and deformable object manipulation.
            We extensively test nine common and state-of-the-art VLM families (two closed-source, seven open-source) on our benchmark, including variants to test different model sizes.
            The performance of VLMs significantly varies across tasks, and this trend persists in our real-world low-level manipulation tasks.
            It also shows that there remains a significant gap between these models and human-level understanding.
            We conclude with lessons learned from the benchmark.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  
  </div>

</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">

      <!-- Types of Questions in ManipBench -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Types of Questions in ManipBench</h2>
          <p class="has-text-justified">
            ManipBench includes a total of <b>14156</b> multiple choice questions to evaluate the reasoning capabilities of the VLMs as robotic manipulation agents.
            These questions are spread across numerous categories and dimensions as shown in the table below.
            Hover over the different <b>Question Types / Tasks</b> to know more about what those questions are evaluating.
          </p>

          <div class="table-container">
            <table class="table is-striped is-fullwidth">
              <thead>
                <tr>
                  <th style="border-bottom: 1.5px solid black; vertical-align: middle; text-align: center;">Category</th>
                  <th style="border-bottom: 1.5px solid black; vertical-align: middle; text-align: center;">Question Types / Tasks</th>
                  <th style="border-bottom: 1.5px solid black; vertical-align: middle; text-align: center;">Number of Questions</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td rowspan="3" style="vertical-align: middle; text-align: center;">
                    <strong>From Public Robotic Manipulation Datasets <br>(Question Type 1)</strong>
                  </td>
                  <td  class="tooltip-row" data-tooltip="Agent chooses the correct trajectory information given scene images and task descriptions obtained from DROID pick and place tasks.">DROID pick and place</td>
                  <td>2020</td>
                </tr>
                <tr>
                  <td class="tooltip-row" data-tooltip="Agent chooses the correct trajectory information given scene images and task descriptions obtained from DROID articulated tasks.">DROID articulated</td>
                  <td>1640</td>
                </tr>
                <tr style="border-bottom: 1.5px solid black;">
                  <td class="tooltip" data-tooltip="Agent chooses the correct trajectory information given scene images and task descriptions obtained from Bridge data.">Bridge</td>
                  <td>3540</td>
                </tr>
                <tr>
                  <td rowspan="3" style="vertical-align: middle; text-align: center;">
                    <strong>From Public Robotic Manipulation Datasets <br>(Question Type 2)</strong>
                  </td>
                  <td class="tooltip" data-tooltip="Agent chooses the correct pick and place points separately given scene images and task descriptions obtained from DROID pick and place tasks.">DROID pick and place</td>
                  <td>1010</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Agent chooses the correct pick and place points separately given scene images and task descriptions obtained from DROID articulated tasks.">DROID articulated</td>
                  <td>820</td>
                </tr>
                <tr style="border-bottom: 1.5px solid black;">
                  <td class="tooltip" data-tooltip="Agent chooses the correct pick and place points separately given scene images and task descriptions obtained from Bridge data.">Bridge</td>
                  <td>1770</td>
                </tr>
                <tr>
                  <td rowspan="10" style="vertical-align: middle; text-align: center;">
                    <strong>For Evaluating Fabric Manipulation (Manually Curated)</strong>
                  </td>
                  <td class="tooltip" data-tooltip="Evaluates physical reasoning capabilities focused on human intuition. Given four choices in natural language, the VLM has to pick the correct one based on common-sense.">Task Planning Understanding</td>
                  <td>240</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Evaluates agent's ability to infer the correct fabric state. Given an image observation and four choices describing the state (e.g., lying flat, slightly crumpled), the VLM has to pick the correct one.">Fabric State Understanding</td>
                  <td>234</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Evaluates agent's understanding of directions by noting the location of fabric corners in the scene (e.g., “bottom-right”), which the model has to correctly identify from an image of a flat fabric.">Spatial Reasoning Abilities</td>
                  <td>325</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Evaluates agent's ability to map key-points in the image observation to the correct grid location.">Keypoint Mapping Abilities</td>
                  <td>312</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Evaluates temporal understanding capabilities by prompting VLMs to re-arrange a shuffled sequence of image observations from a single task roll-out, based on the sequence of fabric states after each action. The model has to choose the correct option from a list of different permutations of the sequence.">Temporal Understanding of Action Sequence</td>
                  <td>240</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Evaluates agent's understanding of the impact of short versus long pick-place actions in achieving the desired fabric configuration from the initial configuration provided as an image observation.">Action Length Understanding</td>
                  <td>240</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Evaluates agent's ability to predict the correct pick-place action from four choices of robot actions to achieve a specified state transition, based on initial and final image observations.">Inverse Dynamics Understanding</td>
                  <td>240</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Evaluates agent's understanding of interactions between a fabric and solid objects in the scene. Given an image observation of a scene, consisting of a single fabric and multiple solid objects, the VLM has to choose the correct pick-place action based on the specified constraints in natural language.">Fabric-Solid Body Interaction Understanding</td>
                  <td>282</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Evaluates agent's understanding of interactions between multiple fabrics, particularly when performing bi-manual actions. Given an image observation of a scene, consisting of multiple fabrics lying on a table, the VLM has to choose the correct action based on the specified constraints in natural language.">Fabric-Fabric Interaction Understanding</td>
                  <td>280</td>
                </tr>
                <tr style="border-bottom: 1.5px solid black;">
                  <td class="tooltip" data-tooltip="Evaluates agent's counterfactual reasoning abilities using ground-truth data from other dimensions. Given an image observation and a description of a particular robot action, along with its impact, the VLM has to correctly reason about the consequences of any changes in the scene and/or robot action.">Counter Factual Understanding</td>
                  <td>269</td>
                </tr>
                <tr>
                  <td rowspan="4" style="vertical-align: middle; text-align: center;">
                    <strong>From Existing Simulation Environments</strong>
                  </td>
                  <td class="tooltip" data-tooltip="Agent chooses the correct key-point information given scene images and task descriptions obtained for the Place Carrot task from the SimplerEnv benchmark.">Place carrot<br>(pick and place task)</td>
                  <td>277</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Agent chooses the correct key-point information given scene images and task descriptions obtained for the Close Drawer task from the SimplerEnv benchmark.">Close Drawer<br>(articulated manipulation task)</td>
                  <td>83</td>
                </tr>
                <tr>
                  <td class="tooltip" data-tooltip="Agent chooses the correct key-point information given scene images and task descriptions obtained for the Straighten Rope task from the Softgym benchmark.">Straighten Rope<br>(deformable manipulation task)</td>
                  <td>140</td>
                </tr>
                <tr style="border-bottom: 1.5px solid black;">
                  <td class="tooltip" data-tooltip="Agent chooses the correct key-point information given scene images and task descriptions obtained for the Sweep Object task from the RLBench benchmark.">Sweep Object<br>(tool manipulation task)</td>
                  <td>194</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>
      <!-- /Types of Questions in ManipBench -->

    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title has-text-centered">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
